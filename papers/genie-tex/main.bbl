\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Babuschkin et~al.(2010)Babuschkin, Baumli, Bell, Bhupatiraju, Bruce,
  Buchlovsky, Budden, Cai, Clark, Danihelka, et~al.]{babuschkin2010deepmind}
I.~Babuschkin, K.~Baumli, A.~Bell, S.~Bhupatiraju, J.~Bruce, P.~Buchlovsky,
  D.~Budden, T.~Cai, A.~Clark, I.~Danihelka, et~al.
\newblock The deepmind jax ecosystem, 2020.
\newblock \emph{URL http://github. com/deepmind}, 2010.

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{webvid}
M.~Bain, A.~Nagrani, G.~Varol, and A.~Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In \emph{2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 1708--1718, Los Alamitos, CA, USA, oct 2021. IEEE Computer
  Society.
\newblock \doi{10.1109/ICCV48922.2021.00175}.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokov, Huizinga, Tang, Ecoffet,
  Houghton, Sampedro, and Clune]{baker2022video}
B.~Baker, I.~Akkaya, P.~Zhokov, J.~Huizinga, J.~Tang, A.~Ecoffet, B.~Houghton,
  R.~Sampedro, and J.~Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online
  videos.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24639--24654, 2022.

\bibitem[Bamford and Lucas(2020)]{bamfordnge2020}
C.~Bamford and S.~M. Lucas.
\newblock Neural game engine: Accurate learning ofgeneralizable forward models
  from pixels.
\newblock In \emph{Conference on Games}, 2020.

\bibitem[Bauer et~al.(2023)Bauer, Baumli, Behbahani, Bhoopchand,
  Bradley-Schmieg, Chang, Clay, Collister, Dasagi, Gonzalez, Gregor, Hughes,
  Kashem, Loks-Thompson, Openshaw, Parker-Holder, Pathak, Perez-Nieves,
  Rakicevic, Rockt\"{a}schel, Schroecker, Singh, Sygnowski, Tuyls, York,
  Zacherl, and Zhang]{ada}
J.~Bauer, K.~Baumli, F.~Behbahani, A.~Bhoopchand, N.~Bradley-Schmieg, M.~Chang,
  N.~Clay, A.~Collister, V.~Dasagi, L.~Gonzalez, K.~Gregor, E.~Hughes,
  S.~Kashem, M.~Loks-Thompson, H.~Openshaw, J.~Parker-Holder, S.~Pathak,
  N.~Perez-Nieves, N.~Rakicevic, T.~Rockt\"{a}schel, Y.~Schroecker, S.~Singh,
  J.~Sygnowski, K.~Tuyls, S.~York, A.~Zacherl, and L.~M. Zhang.
\newblock Human-timescale adaptation in an open-ended task space.
\newblock In A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and
  J.~Scarlett, editors, \emph{Proceedings of the 40th International Conference
  on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning
  Research}, pages 1887--1935. PMLR, 23--29 Jul 2023.

\bibitem[Blattmann et~al.(2023{\natexlab{a}})Blattmann, Dockhorn, Kulal,
  Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, Jampani, and
  Rombach]{blattmann2023stable}
A.~Blattmann, T.~Dockhorn, S.~Kulal, D.~Mendelevitch, M.~Kilian, D.~Lorenz,
  Y.~Levi, Z.~English, V.~Voleti, A.~Letts, V.~Jampani, and R.~Rombach.
\newblock Stable video diffusion: Scaling latent video diffusion models to
  large datasets, 2023{\natexlab{a}}.

\bibitem[Blattmann et~al.(2023{\natexlab{b}})Blattmann, Rombach, Ling,
  Dockhorn, Kim, Fidler, and Kreis]{Blattmann2023AlignYL}
A.~Blattmann, R.~Rombach, H.~Ling, T.~Dockhorn, S.~W. Kim, S.~Fidler, and
  K.~Kreis.
\newblock Align your latents: High-resolution video synthesis with latent
  diffusion models.
\newblock \emph{2023 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 22563--22575, 2023{\natexlab{b}}.

\bibitem[Brohan et~al.(2023)Brohan, Brown, Carbajal, Chebotar, Dabis, Finn,
  Gopalakrishnan, Hausman, Herzog, Hsu, Ibarz, Ichter, Irpan, Jackson,
  Jesmonth, Joshi, Julian, Kalashnikov, Kuang, Leal, Lee, Levine, Lu, Malla,
  Manjunath, Mordatch, Nachum, Parada, Peralta, Perez, Pertsch, Quiambao, Rao,
  Ryoo, Salazar, Sanketi, Sayed, Singh, Sontakke, Stone, Tan, Tran, Vanhoucke,
  Vega, Vuong, Xia, Xiao, Xu, Xu, Yu, and Zitkovich]{brohan2023rt1}
A.~Brohan, N.~Brown, J.~Carbajal, Y.~Chebotar, J.~Dabis, C.~Finn,
  K.~Gopalakrishnan, K.~Hausman, A.~Herzog, J.~Hsu, J.~Ibarz, B.~Ichter,
  A.~Irpan, T.~Jackson, S.~Jesmonth, N.~J. Joshi, R.~Julian, D.~Kalashnikov,
  Y.~Kuang, I.~Leal, K.-H. Lee, S.~Levine, Y.~Lu, U.~Malla, D.~Manjunath,
  I.~Mordatch, O.~Nachum, C.~Parada, J.~Peralta, E.~Perez, K.~Pertsch,
  J.~Quiambao, K.~Rao, M.~Ryoo, G.~Salazar, P.~Sanketi, K.~Sayed, J.~Singh,
  S.~Sontakke, A.~Stone, C.~Tan, H.~Tran, V.~Vanhoucke, S.~Vega, Q.~Vuong,
  F.~Xia, T.~Xiao, P.~Xu, S.~Xu, T.~Yu, and B.~Zitkovich.
\newblock Rt-1: Robotics transformer for real-world control at scale.
\newblock In \emph{Robotics: Science and Systems}, 2023.

\bibitem[Brooks et~al.(2024)Brooks, Peebles, Homes, DePue, Guo, Jing, Schnurr,
  Taylor, Luhman, Luhman, Ng, Wang, and Ramesh]{videoworldsimulators2024}
T.~Brooks, B.~Peebles, C.~Homes, W.~DePue, Y.~Guo, L.~Jing, D.~Schnurr,
  J.~Taylor, T.~Luhman, E.~Luhman, C.~W.~Y. Ng, R.~Wang, and A.~Ramesh.
\newblock Video generation models as world simulators.
\newblock 2024.
\newblock URL
  \url{https://openai.com/research/video-generation-models-as-world-simulators}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chang et~al.(2022)Chang, Zhang, Jiang, Liu, and
  Freeman]{Chang_2022_CVPR}
H.~Chang, H.~Zhang, L.~Jiang, C.~Liu, and W.~T. Freeman.
\newblock Maskgit: Masked generative image transformer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 11315--11325, June 2022.

\bibitem[Chiappa et~al.(2017)Chiappa, Racaniere, Wierstra, and
  Mohamed]{chiappa2017recurrent}
S.~Chiappa, S.~Racaniere, D.~Wierstra, and S.~Mohamed.
\newblock Recurrent environment simulators.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Clark et~al.(2019)Clark, Donahue, and Simonyan]{dvdgan}
A.~Clark, J.~Donahue, and K.~Simonyan.
\newblock Efficient video generation on complex datasets.
\newblock \emph{CoRR}, abs/1907.06571, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.06571}.

\bibitem[Clune(2019)]{clune2019ai}
J.~Clune.
\newblock Ai-gas: Ai-generating algorithms, an alternate paradigm for producing
  general artificial intelligence.
\newblock \emph{arXiv preprint arXiv:1905.10985}, 2019.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and Schulman]{procgen}
K.~Cobbe, C.~Hesse, J.~Hilton, and J.~Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 2048--2056, 2020.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, Jenatton, Beyer, Tschannen,
  Arnab, Wang, Riquelme~Ruiz, Minderer, Puigcerver, Evci, Kumar, Steenkiste,
  Elsayed, Mahendran, Yu, Oliver, Huot, Bastings, Collier, Gritsenko, Birodkar,
  Vasconcelos, Tay, Mensink, Kolesnikov, Pavetic, Tran, Kipf, Lucic, Zhai,
  Keysers, Harmsen, and Houlsby]{pmlr-v202-dehghani23a}
M.~Dehghani, J.~Djolonga, B.~Mustafa, P.~Padlewski, J.~Heek, J.~Gilmer, A.~P.
  Steiner, M.~Caron, R.~Geirhos, I.~Alabdulmohsin, R.~Jenatton, L.~Beyer,
  M.~Tschannen, A.~Arnab, X.~Wang, C.~Riquelme~Ruiz, M.~Minderer,
  J.~Puigcerver, U.~Evci, M.~Kumar, S.~V. Steenkiste, G.~F. Elsayed,
  A.~Mahendran, F.~Yu, A.~Oliver, F.~Huot, J.~Bastings, M.~Collier, A.~A.
  Gritsenko, V.~Birodkar, C.~N. Vasconcelos, Y.~Tay, T.~Mensink, A.~Kolesnikov,
  F.~Pavetic, D.~Tran, T.~Kipf, M.~Lucic, X.~Zhai, D.~Keysers, J.~J. Harmsen,
  and N.~Houlsby.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and
  J.~Scarlett, editors, \emph{Proceedings of the 40th International Conference
  on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning
  Research}, pages 7480--7512. PMLR, 23--29 Jul 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Edwards et~al.(2019)Edwards, Sahni, Schroecker, and
  Isbell]{edwards2019imitating}
A.~Edwards, H.~Sahni, Y.~Schroecker, and C.~Isbell.
\newblock Imitating latent policies from observation.
\newblock In \emph{International conference on machine learning}, pages
  1755--1763. PMLR, 2019.

\bibitem[Eslami et~al.(2018)Eslami, Rezende, Besse, Viola, Morcos, Garnelo,
  Ruderman, Rusu, Danihelka, Gregor, Reichert, Buesing, Weber, Vinyals,
  Rosenbaum, Rabinowitz, King, Hillier, Botvinick, Wierstra, Kavukcuoglu, and
  Hassabis]{scienceaar6170}
S.~M.~A. Eslami, D.~J. Rezende, F.~Besse, F.~Viola, A.~S. Morcos, M.~Garnelo,
  A.~Ruderman, A.~A. Rusu, I.~Danihelka, K.~Gregor, D.~P. Reichert, L.~Buesing,
  T.~Weber, O.~Vinyals, D.~Rosenbaum, N.~Rabinowitz, H.~King, C.~Hillier,
  M.~Botvinick, D.~Wierstra, K.~Kavukcuoglu, and D.~Hassabis.
\newblock Neural scene representation and rendering.
\newblock \emph{Science}, 360\penalty0 (6394):\penalty0 1204--1210, 2018.
\newblock \doi{10.1126/science.aar6170}.

\bibitem[Esser et~al.(2023)Esser, Chiu, Atighehchian, Granskog, and
  Germanidis]{esser2023structure}
P.~Esser, J.~Chiu, P.~Atighehchian, J.~Granskog, and A.~Germanidis.
\newblock Structure and content-guided video synthesis with diffusion models.
\newblock In \emph{2023 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, 2023.

\bibitem[Finn et~al.(2016)Finn, Goodfellow, and Levine]{finn2016}
C.~Finn, I.~Goodfellow, and S.~Levine.
\newblock Unsupervised learning for physical interaction through video
  prediction.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, NIPS'16, page 64–72, Red Hook, NY, USA,
  2016. Curran Associates Inc.
\newblock ISBN 9781510838819.

\bibitem[Gupta et~al.(2023)Gupta, Tian, Zhang, Wu, Mart{\'\i}n-Mart{\'\i}n, and
  Fei-Fei]{gupta2023maskvit}
A.~Gupta, S.~Tian, Y.~Zhang, J.~Wu, R.~Mart{\'\i}n-Mart{\'\i}n, and L.~Fei-Fei.
\newblock Maskvit: Masked visual pre-training for video prediction.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Ha and Schmidhuber(2018)]{worldmodels}
D.~Ha and J.~Schmidhuber.
\newblock Recurrent world models facilitate policy evolution.
\newblock In \emph{Proceedings of the 32Nd International Conference on Neural
  Information Processing Systems}, NeurIPS'18, pages 2455--2467, 2018.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and Norouzi]{dreamer}
D.~Hafner, T.~Lillicrap, J.~Ba, and M.~Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hafner et~al.(2021)Hafner, Lillicrap, Norouzi, and
  Ba]{hafner2021mastering}
D.~Hafner, T.~P. Lillicrap, M.~Norouzi, and J.~Ba.
\newblock Mastering atari with discrete world models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Henry et~al.(2020)Henry, Dachapally, Pawar, and
  Chen]{henry-etal-2020-query}
A.~Henry, P.~R. Dachapally, S.~S. Pawar, and Y.~Chen.
\newblock Query-key normalization for transformers.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 4246--4253, Online, Nov. 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.379}.

\bibitem[Ho et~al.(2022{\natexlab{a}})Ho, Chan, Saharia, Whang, Gao, Gritsenko,
  Kingma, Poole, Norouzi, Fleet, and Salimans]{ho2022imagen}
J.~Ho, W.~Chan, C.~Saharia, J.~Whang, R.~Gao, A.~Gritsenko, D.~P. Kingma,
  B.~Poole, M.~Norouzi, D.~J. Fleet, and T.~Salimans.
\newblock Imagen video: High definition video generation with diffusion models,
  2022{\natexlab{a}}.

\bibitem[Ho et~al.(2022{\natexlab{b}})Ho, Salimans, Gritsenko, Chan, Norouzi,
  and Fleet]{NEURIPS2022_39235c56}
J.~Ho, T.~Salimans, A.~Gritsenko, W.~Chan, M.~Norouzi, and D.~J. Fleet.
\newblock Video diffusion models.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 8633--8646. Curran Associates, Inc., 2022{\natexlab{b}}.

\bibitem[Hong et~al.(2022)Hong, Ding, Zheng, Liu, and Tang]{hong2022cogvideo}
W.~Hong, M.~Ding, W.~Zheng, X.~Liu, and J.~Tang.
\newblock Cogvideo: Large-scale pretraining for text-to-video generation via
  transformers.
\newblock \emph{arXiv preprint arXiv:2205.15868}, 2022.

\bibitem[Hong et~al.(2023)Hong, Ding, Zheng, Liu, and Tang]{hong2023cogvideo}
W.~Hong, M.~Ding, W.~Zheng, X.~Liu, and J.~Tang.
\newblock Cogvideo: Large-scale pretraining for text-to-video generation via
  transformers.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=rB6TpjAuSRy}.

\bibitem[H{\"o}ppe et~al.(2022)H{\"o}ppe, Mehrjou, Bauer, Nielsen, and
  Dittadi]{hoppe2022diffusion}
T.~H{\"o}ppe, A.~Mehrjou, S.~Bauer, D.~Nielsen, and A.~Dittadi.
\newblock Diffusion models for video prediction and infilling.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.

\bibitem[Hu et~al.(2023)Hu, Russell, Yeo, Murez, Fedoseev, Kendall, Shotton,
  and Corrado]{hu2023gaia1}
A.~Hu, L.~Russell, H.~Yeo, Z.~Murez, G.~Fedoseev, A.~Kendall, J.~Shotton, and
  G.~Corrado.
\newblock Gaia-1: A generative world model for autonomous driving, 2023.

\bibitem[Huang et~al.(2022)Huang, Jin, Yi, and Sigal]{layered2022}
J.~Huang, Y.~Jin, K.~M. Yi, and L.~Sigal.
\newblock Layered controllable video generation.
\newblock In \emph{Computer Vision – ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23–27, 2022, Proceedings, Part XVI}, page 546–564,
  Berlin, Heidelberg, 2022. Springer-Verlag.
\newblock ISBN 978-3-031-19786-4.

\bibitem[Jouppi et~al.(2020)Jouppi, Yoon, Kurian, Li, Patil, Laudon, Young, and
  Patterson]{jouppi2020domain}
N.~P. Jouppi, D.~H. Yoon, G.~Kurian, S.~Li, N.~Patil, J.~Laudon, C.~Young, and
  D.~Patterson.
\newblock A domain-specific supercomputer for training deep neural networks.
\newblock \emph{Communications of the ACM}, 63\penalty0 (7):\penalty0 67--78,
  2020.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
D.~Kalashnikov, A.~Irpan, P.~Pastor, J.~Ibarz, A.~Herzog, E.~Jang, D.~Quillen,
  E.~Holly, M.~Kalakrishnan, V.~Vanhoucke, et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Kalchbrenner et~al.(2017)Kalchbrenner, van~den Oord, Simonyan,
  Danihelka, Vinyals, Graves, and Kavukcuoglu]{vpnkalchbrenner17a}
N.~Kalchbrenner, A.~van~den Oord, K.~Simonyan, I.~Danihelka, O.~Vinyals,
  A.~Graves, and K.~Kavukcuoglu.
\newblock Video pixel networks.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pages 1771--1779. PMLR, 06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/kalchbrenner17a.html}.

\bibitem[Kapturowski et~al.(2018)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{kapturowski2018recurrent}
S.~Kapturowski, G.~Ostrovski, J.~Quan, R.~Munos, and W.~Dabney.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{International conference on learning representations}, 2018.

\bibitem[Kim et~al.(2020)Kim, Zhou, Philion, Torralba, and
  Fidler]{Kim_2020_CVPR}
S.~W. Kim, Y.~Zhou, J.~Philion, A.~Torralba, and S.~Fidler.
\newblock Learning to simulate dynamic environments with gamegan.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Kim et~al.(2021)Kim, Philion, Torralba, and Fidler]{Kim_2021_CVPR}
S.~W. Kim, J.~Philion, A.~Torralba, and S.~Fidler.
\newblock Drivegan: Towards a controllable high-quality neural simulation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 5820--5829, June 2021.

\bibitem[Le~Moing et~al.(2021)Le~Moing, Ponce, and
  Schmid]{NEURIPS2021_757b505c}
G.~Le~Moing, J.~Ponce, and C.~Schmid.
\newblock Ccvs: Context-aware controllable video synthesis.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 14042--14055. Curran Associates, Inc., 2021.

\bibitem[Lotter et~al.(2017)Lotter, Kreiman, and Cox]{lotter2017deep}
W.~Lotter, G.~Kreiman, and D.~Cox.
\newblock Deep predictive coding networks for video prediction and unsupervised
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1ewdt9xe}.

\bibitem[Luc et~al.(2020)Luc, Clark, Dieleman, de~Las~Casas, Doron, Cassirer,
  and Simonyan]{trivdgan2020}
P.~Luc, A.~Clark, S.~Dieleman, D.~de~Las~Casas, Y.~Doron, A.~Cassirer, and
  K.~Simonyan.
\newblock Transformation-based adversarial video prediction on large-scale
  data.
\newblock \emph{CoRR}, abs/2003.04035, 2020.

\bibitem[Menapace et~al.(2021)Menapace, Lathuili{\`{e}}re, Tulyakov, Siarohin,
  and Ricci]{menapace2021}
W.~Menapace, S.~Lathuili{\`{e}}re, S.~Tulyakov, A.~Siarohin, and E.~Ricci.
\newblock Playable video generation.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2021, virtual, June 19-25, 2021}, pages 10061--10070.
  Computer Vision Foundation / {IEEE}, 2021.

\bibitem[Menapace et~al.(2022)Menapace, Lathuilière, Siarohin, Theobalt,
  Tulyakov, Golyanik, and Ricci]{Menapace2022PlayableEnvironments}
W.~Menapace, S.~Lathuilière, A.~Siarohin, C.~Theobalt, S.~Tulyakov,
  V.~Golyanik, and E.~Ricci.
\newblock Playable environments: Video manipulation in space and time.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2022.

\bibitem[Micheli et~al.(2023)Micheli, Alonso, and
  Fleuret]{micheli2023transformers}
V.~Micheli, E.~Alonso, and F.~Fleuret.
\newblock Transformers are sample-efficient world models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Nunes et~al.(2020)Nunes, Dehban, Moreno, and
  Santos-Victor]{actionsurvey}
M.~S. Nunes, A.~Dehban, P.~Moreno, and J.~Santos-Victor.
\newblock Action-conditioned benchmarking of robotic video prediction models: a
  comparative study.
\newblock In \emph{2020 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 8316--8322, 2020.
\newblock \doi{10.1109/ICRA40945.2020.9196839}.

\bibitem[Oh et~al.(2015)Oh, Guo, Lee, Lewis, and Singh]{oh2015}
J.~Oh, X.~Guo, H.~Lee, R.~Lewis, and S.~Singh.
\newblock Action-conditional video prediction using deep networks in atari
  games.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems - Volume 2}, NIPS'15, page 2863–2871,
  Cambridge, MA, USA, 2015. MIT Press.

\bibitem[{Open Ended Learning Team} et~al.(2021){Open Ended Learning Team},
  Stooke, Mahajan, Barros, Deck, Bauer, Sygnowski, Trebacz, Jaderberg, Mathieu,
  McAleese, Bradley{-}Schmieg, Wong, Porcel, Raileanu, Hughes{-}Fitt, Dalibard,
  and Czarnecki]{xland}
{Open Ended Learning Team}, A.~Stooke, A.~Mahajan, C.~Barros, C.~Deck,
  J.~Bauer, J.~Sygnowski, M.~Trebacz, M.~Jaderberg, M.~Mathieu, N.~McAleese,
  N.~Bradley{-}Schmieg, N.~Wong, N.~Porcel, R.~Raileanu, S.~Hughes{-}Fitt,
  V.~Dalibard, and W.~M. Czarnecki.
\newblock Open-ended learning leads to generally capable agents.
\newblock \emph{CoRR}, abs/2107.12808, 2021.

\bibitem[Oquab et~al.(2023)Oquab, Darcet, Moutakanni, Vo, Szafraniec, Khalidov,
  Fernandez, Haziza, Massa, El-Nouby, et~al.]{oquab2023dinov2}
M.~Oquab, T.~Darcet, T.~Moutakanni, H.~Vo, M.~Szafraniec, V.~Khalidov,
  P.~Fernandez, D.~Haziza, F.~Massa, A.~El-Nouby, et~al.
\newblock Dinov2: Learning robust visual features without supervision.
\newblock \emph{arXiv preprint arXiv:2304.07193}, 2023.

\bibitem[Pan et~al.(2022)Pan, Zhu, Wang, and Yang]{NEURIPS2022_9316769a}
M.~Pan, X.~Zhu, Y.~Wang, and X.~Yang.
\newblock Iso-dream: Isolating and leveraging noncontrollable visual dynamics
  in world models.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 23178--23191. Curran Associates, Inc., 2022.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, and I.~Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2020zero}
S.~Rajbhandari, J.~Rasley, O.~Ruwase, and Y.~He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh21a}
A.~Ramesh, M.~Pavlov, G.~Goh, S.~Gray, C.~Voss, A.~Radford, M.~Chen, and
  I.~Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 8821--8831. PMLR,
  18--24 Jul 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
A.~Ramesh, P.~Dhariwal, A.~Nichol, C.~Chu, and M.~Chen.
\newblock Hierarchical text-conditional image generation with clip latents,
  2022.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-maron, Gim{\'e}nez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi,
  Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and de~Freitas]{reed2022a}
S.~Reed, K.~Zolna, E.~Parisotto, S.~G. Colmenarejo, A.~Novikov, G.~Barth-maron,
  M.~Gim{\'e}nez, Y.~Sulsky, J.~Kay, J.~T. Springenberg, T.~Eccles, J.~Bruce,
  A.~Razavi, A.~Edwards, N.~Heess, Y.~Chen, R.~Hadsell, O.~Vinyals, M.~Bordbar,
  and N.~de~Freitas.
\newblock A generalist agent.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock Featured Certification, Outstanding Certification.

\bibitem[Risi and Togelius(2020{\natexlab{a}})]{pcg}
S.~Risi and J.~Togelius.
\newblock Increasing generality in machine learning through procedural content
  generation.
\newblock \emph{Nature Machine Intelligence}, 2, 08 2020{\natexlab{a}}.
\newblock \doi{10.1038/s42256-020-0208-z}.

\bibitem[Risi and Togelius(2020{\natexlab{b}})]{pcg2}
S.~Risi and J.~Togelius.
\newblock Procedural content generation: From automatically generating game
  levels to increasing generality in machine learning.
\newblock \emph{Nature}, 2020{\natexlab{b}}.

\bibitem[Robine et~al.(2023)Robine, H{\"o}ftmann, Uelwer, and
  Harmeling]{robine2023transformerbased}
J.~Robine, M.~H{\"o}ftmann, T.~Uelwer, and S.~Harmeling.
\newblock Transformer-based world models are happy with 100k interactions.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{Rombach_2022_CVPR}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 10684--10695, June 2022.

\bibitem[Rybkin* et~al.(2019)Rybkin*, Pertsch*, Derpanis, Daniilidis, and
  Jaegle]{rybkin2019learning}
O.~Rybkin*, K.~Pertsch*, K.~G. Derpanis, K.~Daniilidis, and A.~Jaegle.
\newblock Learning what you can do before doing anything.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Gontijo-Lopes, Ayan, Salimans, Ho, Fleet, and
  Norouzi]{saharia2022photorealistic}
C.~Saharia, W.~Chan, S.~Saxena, L.~Li, J.~Whang, E.~Denton, S.~K.~S.
  Ghasemipour, R.~Gontijo-Lopes, B.~K. Ayan, T.~Salimans, J.~Ho, D.~J. Fleet,
  and M.~Norouzi.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, editors,
  \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Schmidt and Jiang(2024)]{schmidt2024learning}
D.~Schmidt and M.~Jiang.
\newblock Learning to act without actions.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{megatron}
M.~Shoeybi, M.~Patwary, R.~Puri, P.~LeGresley, J.~Casper, and B.~Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{CoRR}, abs/1909.08053, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.08053}.

\bibitem[Singer et~al.(2023)Singer, Polyak, Hayes, Yin, An, Zhang, Hu, Yang,
  Ashual, Gafni, Parikh, Gupta, and Taigman]{singer2023makeavideo}
U.~Singer, A.~Polyak, T.~Hayes, X.~Yin, J.~An, S.~Zhang, Q.~Hu, H.~Yang,
  O.~Ashual, O.~Gafni, D.~Parikh, S.~Gupta, and Y.~Taigman.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Sudhakaran et~al.(2023)Sudhakaran, Gonz{\'a}lez-Duque, Glanois,
  Freiberger, Najarro, and Risi]{sudhakaran2023prompt}
S.~Sudhakaran, M.~Gonz{\'a}lez-Duque, C.~Glanois, M.~Freiberger, E.~Najarro,
  and S.~Risi.
\newblock Prompt-guided level generation.
\newblock In \emph{Proceedings of the Companion Conference on Genetic and
  Evolutionary Computation}, pages 179--182, 2023.

\bibitem[Summerville et~al.(2018)Summerville, Snodgrass, Guzdial, Holmg{\aa}rd,
  Hoover, Isaksen, Nealen, and Togelius]{pcgml}
A.~Summerville, S.~Snodgrass, M.~Guzdial, C.~Holmg{\aa}rd, A.~K. Hoover,
  A.~Isaksen, A.~Nealen, and J.~Togelius.
\newblock Procedural content generation via machine learning {(PCGML)}.
\newblock \emph{{IEEE} Trans. Games}, 10\penalty0 (3):\penalty0 257--270, 2018.

\bibitem[Todd et~al.(2023)Todd, Earle, Nasir, Green, and
  Togelius]{todd2023level}
G.~Todd, S.~Earle, M.~U. Nasir, M.~C. Green, and J.~Togelius.
\newblock Level generation through large language models.
\newblock In \emph{Proceedings of the 18th International Conference on the
  Foundations of Digital Games}, pages 1--8, 2023.

\bibitem[Torabi et~al.(2018)Torabi, Warnell, and Stone]{torabi2018behavioral}
F.~Torabi, G.~Warnell, and P.~Stone.
\newblock Behavioral cloning from observation.
\newblock \emph{arXiv preprint arXiv:1805.01954}, 2018.

\bibitem[Unterthiner et~al.(2019)Unterthiner, van Steenkiste, Kurach, Marinier,
  Michalski, and Gelly]{unterthiner2019fvd}
T.~Unterthiner, S.~van Steenkiste, K.~Kurach, R.~Marinier, M.~Michalski, and
  S.~Gelly.
\newblock {FVD}: A new metric for video generation, 2019.

\bibitem[van~den Oord et~al.()van~den Oord, Razavi, Uria, Çağlar Ünlü,
  Nash, Wolff, Durkan, Ding, Górny, Gladchenko, Riedel, Qi, Kelly, Bauer,
  Donahue, Zhang, Malinowski, Bińkowski, Luc, Riachi, Strudel,
  Sander~Dieleman, Ganin, and Eaton-Rosen.]{imagen2}
A.~van~den Oord, A.~Razavi, B.~Uria, Çağlar Ünlü, C.~Nash, C.~Wolff,
  C.~Durkan, D.~Ding, D.~Górny, E.~Gladchenko, F.~Riedel, H.~Qi, J.~Kelly,
  J.~Bauer, J.~Donahue, J.~Zhang, M.~Malinowski, M.~Bińkowski, P.~Luc,
  R.~Riachi, R.~Strudel, T.~P.~I. Sander~Dieleman, Y.~Ganin, and
  Z.~Eaton-Rosen.
\newblock Imagen 2.
\newblock URL \url{https://deepmind.google/technologies/imagen-2/}.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and
  Kavukcuoglu]{vqvae2017}
A.~van~den Oord, O.~Vinyals, and K.~Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, page 6309–6318, Red Hook, NY,
  USA, 2017. Curran Associates Inc.
\newblock ISBN 9781510860964.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[Villegas et~al.(2023)Villegas, Babaeizadeh, Kindermans, Moraldo,
  Zhang, Saffar, Castro, Kunze, and Erhan]{villegas2023phenaki}
R.~Villegas, M.~Babaeizadeh, P.-J. Kindermans, H.~Moraldo, H.~Zhang, M.~T.
  Saffar, S.~Castro, J.~Kunze, and D.~Erhan.
\newblock Phenaki: Variable length video generation from open domain textual
  descriptions.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Walker et~al.(2021)Walker, Razavi, and van~den
  Oord]{walker2021predicting}
J.~C. Walker, A.~Razavi, and A.~van~den Oord.
\newblock Predicting video with {VQVAE}, 2021.

\bibitem[Wang et~al.(2023)Wang, He, Li, Li, Yu, Ma, Chen, Wang, Luo, Liu, Wang,
  Wang, and Qiao]{wang2023internvid}
Y.~Wang, Y.~He, Y.~Li, K.~Li, J.~Yu, X.~Ma, X.~Chen, Y.~Wang, P.~Luo, Z.~Liu,
  Y.~Wang, L.~Wang, and Y.~Qiao.
\newblock Internvid: A large-scale video-text dataset for multimodal
  understanding and generation, 2023.

\bibitem[Wong et~al.(2023)Wong, Grand, Lew, Goodman, Mansinghka, Andreas, and
  Tenenbaum]{wong2023word}
L.~Wong, G.~Grand, A.~K. Lew, N.~D. Goodman, V.~K. Mansinghka, J.~Andreas, and
  J.~B. Tenenbaum.
\newblock From word models to world models: Translating from natural language
  to the probabilistic language of thought, 2023.

\bibitem[Wu et~al.(2022)Wu, Liang, Ji, Yang, Fang, Jiang, and Duan]{wu2022nuwa}
C.~Wu, J.~Liang, L.~Ji, F.~Yang, Y.~Fang, D.~Jiang, and N.~Duan.
\newblock N{\"u}wa: Visual synthesis pre-training for neural visual world
  creation.
\newblock In \emph{European conference on computer vision}, pages 720--736.
  Springer, 2022.

\bibitem[Xu et~al.(2020)Xu, Dai, Liu, Gao, Lin, Qi, and Xiong]{xu2020spatial}
M.~Xu, W.~Dai, C.~Liu, X.~Gao, W.~Lin, G.-J. Qi, and H.~Xiong.
\newblock Spatial-temporal transformer networks for traffic flow forecasting.
\newblock \emph{arXiv preprint arXiv:2001.02908}, 2020.

\bibitem[Yan et~al.(2021)Yan, Zhang, Abbeel, and Srinivas]{yan2021videogpt}
W.~Yan, Y.~Zhang, P.~Abbeel, and A.~Srinivas.
\newblock Videogpt: Video generation using vq-vae and transformers, 2021.

\bibitem[Yan et~al.(2023)Yan, Hafner, James, and Abbeel]{teco}
W.~Yan, D.~Hafner, S.~James, and P.~Abbeel.
\newblock Temporally consistent transformers for video generation.
\newblock In A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and
  J.~Scarlett, editors, \emph{Proceedings of the 40th International Conference
  on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning
  Research}, pages 39062--39098. PMLR, 23--29 Jul 2023.

\bibitem[Yang et~al.(2023)Yang, Du, Ghasemipour, Tompson, Schuurmans, and
  Abbeel]{yang2023learning}
M.~Yang, Y.~Du, K.~Ghasemipour, J.~Tompson, D.~Schuurmans, and P.~Abbeel.
\newblock Learning interactive real-world simulators.
\newblock \emph{arXiv preprint arXiv:2310.06114}, 2023.

\bibitem[Ye et~al.(2022)Ye, Zhang, Abbeel, and Gao]{ye2022become}
W.~Ye, Y.~Zhang, P.~Abbeel, and Y.~Gao.
\newblock Become a proficient player with limited data through watching pure
  videos.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Yu et~al.(2023)Yu, Cheng, Sohn, Lezama, Zhang, Chang, Hauptmann, Yang,
  Hao, Essa, and Jiang]{10205485}
L.~Yu, Y.~Cheng, K.~Sohn, J.~Lezama, H.~Zhang, H.~Chang, A.~G. Hauptmann,
  M.~Yang, Y.~Hao, I.~Essa, and L.~Jiang.
\newblock Magvit: Masked generative video transformer.
\newblock In \emph{2023 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 10459--10469, Los Alamitos, CA, USA, jun 2023.
  IEEE Computer Society.
\newblock \doi{10.1109/CVPR52729.2023.01008}.

\end{thebibliography}
